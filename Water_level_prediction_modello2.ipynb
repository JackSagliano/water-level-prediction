{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackSagliano/water-level-prediction/blob/main/Water_level_prediction_modello2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQZ3ClptLWuV"
      },
      "source": [
        "# ðŸŒŠ Water Level Prediction - Versione MEMORY-EFFICIENT\n",
        "## Ottimizzata per RAM limitata\n",
        "\n",
        "Questa versione usa:\n",
        "- **Sampling dei nodi** (non tutti i 5000 ad ogni timestep)\n",
        "- **Dataset on-the-fly** (genera sequenze durante il training)\n",
        "- **Gradienti accumulati** per batch effettivi piÃ¹ grandi\n",
        "\n",
        "**Target:** Battere baseline RMSE = 0.0859"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkfvRe29LaK9"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsrE2gdELUUO",
        "outputId": "3f32fe8a-8aa0-4cd5-8b66-3530fa4f231c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Setup completato!\n",
            "PyTorch: 2.9.0+cu126\n",
            "CUDA: True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Libera memoria\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "print(\"âœ“ Setup completato!\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awo5sCt_LhMz"
      },
      "source": [
        "## 2. Caricamento Dati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usPUXkbfLkYB",
        "outputId": "d3dc9614-a327-4acd-afbc-827ade334fcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Caricamento dati...\n",
            "\n",
            "ERA5 train: (3, 87648, 5, 9)\n",
            "Effemeridi train: (6, 87648)\n",
            "Water level train: (87648, 5000)\n",
            "\n",
            "âœ“ Dati caricati!\n",
            "\n",
            "Variabili separate:\n",
            "  u10: (87648, 5, 9)\n",
            "  v10: (87648, 5, 9)\n",
            "  msl: (87648, 5, 9)\n",
            "  eph: (87648, 6)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/ProgettoDeepLearning/'\n",
        "print(\"Caricamento dati...\\n\")\n",
        "\n",
        "# Carica dati\n",
        "era5_train = np.load(BASE_PATH + 'ERA5_adriatic_u10v10sp_2010-2019.npy')\n",
        "era5_test = np.load(BASE_PATH + 'ERA5_adriatic_u10v10sp_2020.npy')\n",
        "eph_train = np.load(BASE_PATH + 'dist_alt_az_moon-sun_coord13-45_2010-2019_norm.npy')\n",
        "eph_test = np.load(BASE_PATH + 'dist_alt_az_moon-sun_coord13-45_2020_norm.npy')\n",
        "wl_train = np.load(BASE_PATH + 'wl_2010-2020.npy')\n",
        "wl_test = np.load(BASE_PATH + 'wl_2020.npy')\n",
        "lat_vec = np.load(BASE_PATH + 'lat.npy')\n",
        "lon_vec = np.load(BASE_PATH + 'lon.npy')\n",
        "\n",
        "print(f\"ERA5 train: {era5_train.shape}\")\n",
        "print(f\"Effemeridi train: {eph_train.shape}\")\n",
        "print(f\"Water level train: {wl_train.shape}\")\n",
        "print(f\"\\nâœ“ Dati caricati!\")\n",
        "\n",
        "# Separa variabili ERA5\n",
        "train_u10 = era5_train[0]\n",
        "train_v10 = era5_train[1]\n",
        "train_msl = era5_train[2]\n",
        "\n",
        "test_u10 = era5_test[0]\n",
        "test_v10 = era5_test[1]\n",
        "test_msl = era5_test[2]\n",
        "\n",
        "# Transponi effemeridi\n",
        "eph_train = eph_train.T  # (87648, 6)\n",
        "eph_test = eph_test.T\n",
        "\n",
        "print(f\"\\nVariabili separate:\")\n",
        "print(f\"  u10: {train_u10.shape}\")\n",
        "print(f\"  v10: {train_v10.shape}\")\n",
        "print(f\"  msl: {train_msl.shape}\")\n",
        "print(f\"  eph: {eph_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9SkJWB9LlyK",
        "outputId": "42612bb3-e1a9-4fd3-8c0f-18efd1eb5443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "BASELINE: 0.0859\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def RMSE(y_true, y_pred):\n",
        "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
        "\n",
        "baseline_rmse = RMSE(wl_test[:-1], wl_test[1:])\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"BASELINE: {baseline_rmse:.4f}\")\n",
        "print(f\"{'='*60}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYsaoenPLnAE"
      },
      "source": [
        "## 3. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE1LHDMQLozA",
        "outputId": "731a5886-b251-4e74-e44b-ef6532830071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizzazione ERA5...\n",
            "âœ“ ERA5 normalizzato!\n",
            "âœ“ Coordinate normalizzate!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "print(\"Normalizzazione ERA5...\")\n",
        "\n",
        "u10_scaler = StandardScaler()\n",
        "v10_scaler = StandardScaler()\n",
        "msl_scaler = StandardScaler()\n",
        "\n",
        "train_u10_norm = u10_scaler.fit_transform(\n",
        "    train_u10.reshape(train_u10.shape[0], -1)\n",
        ").reshape(train_u10.shape)\n",
        "\n",
        "train_v10_norm = v10_scaler.fit_transform(\n",
        "    train_v10.reshape(train_v10.shape[0], -1)\n",
        ").reshape(train_v10.shape)\n",
        "\n",
        "train_msl_norm = msl_scaler.fit_transform(\n",
        "    train_msl.reshape(train_msl.shape[0], -1)\n",
        ").reshape(train_msl.shape)\n",
        "\n",
        "test_u10_norm = u10_scaler.transform(\n",
        "    test_u10.reshape(test_u10.shape[0], -1)\n",
        ").reshape(test_u10.shape)\n",
        "\n",
        "test_v10_norm = v10_scaler.transform(\n",
        "    test_v10.reshape(test_v10.shape[0], -1)\n",
        ").reshape(test_v10.shape)\n",
        "\n",
        "test_msl_norm = msl_scaler.transform(\n",
        "    test_msl.reshape(test_msl.shape[0], -1)\n",
        ").reshape(test_msl.shape)\n",
        "\n",
        "print(\"âœ“ ERA5 normalizzato!\")\n",
        "\n",
        "# Normalizza coordinate\n",
        "lat_mean, lat_std = lat_vec.mean(), lat_vec.std()\n",
        "lon_mean, lon_std = lon_vec.mean(), lon_vec.std()\n",
        "\n",
        "lat_norm = (lat_vec - lat_mean) / lat_std\n",
        "lon_norm = (lon_vec - lon_mean) / lon_std\n",
        "\n",
        "print(f\"âœ“ Coordinate normalizzate!\")\n",
        "\n",
        "# Libera memoria\n",
        "del era5_train, era5_test, train_u10, train_v10, train_msl, test_u10, test_v10, test_msl\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R64Qe8amLqXA"
      },
      "source": [
        "## 4. Dataset Memory-Efficient\n",
        "\n",
        "**Strategia:**\n",
        "- Non creare tutte le sequenze in memoria\n",
        "- Genera sequenze on-the-fly durante il training\n",
        "- Campiona solo un subset di nodi ad ogni epoca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA1OJBkaLr1W",
        "outputId": "2146d4cc-96c3-4ccd-a5c3-9df8c20702b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creazione Dataset PRO (48h)...\n",
            "Dataset pronto: 3939600 training samples.\n"
          ]
        }
      ],
      "source": [
        "# --- CONFIGURAZIONE PRO ---\n",
        "SEQ_LEN = 48  # 48 ore di memoria (cattura 2 cicli di marea completi)\n",
        "NODES_PER_T = 100 # Aumentato per vedere piÃ¹ costa\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "class MemoryEfficientDataset(Dataset):\n",
        "    def __init__(self, eph, u10, v10, msl, wl, lat, lon, seq_len=48,\n",
        "                 nodes_per_timestep=100):\n",
        "        self.eph = eph\n",
        "        self.u10 = u10\n",
        "        self.v10 = v10\n",
        "        self.msl = msl\n",
        "        self.wl = wl\n",
        "        self.lat = lat\n",
        "        self.lon = lon\n",
        "        self.seq_len = seq_len\n",
        "        self.nodes_per_t = nodes_per_timestep\n",
        "        self.timesteps = wl.shape[0]\n",
        "        self.num_nodes = wl.shape[1]\n",
        "\n",
        "        self.samples = []\n",
        "        # Step ridotto per generare meno campioni ma piÃ¹ significativi\n",
        "        # Generiamo un campione ogni 2 ore per non saturare la RAM con SEQ_LEN 48\n",
        "        for t in range(seq_len, self.timesteps, 2):\n",
        "            sampled_nodes = np.random.choice(self.num_nodes,\n",
        "                                            size=self.nodes_per_t,\n",
        "                                            replace=False)\n",
        "            for n in sampled_nodes:\n",
        "                self.samples.append((t, n))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        t, n = self.samples[idx]\n",
        "\n",
        "        eph_seq = self.eph[t-self.seq_len:t]\n",
        "\n",
        "        # ERA5: (Seq, H, W) per ogni variabile\n",
        "        u10_seq = self.u10[t-self.seq_len:t] # (48, 5, 9)\n",
        "        v10_seq = self.v10[t-self.seq_len:t]\n",
        "        msl_seq = self.msl[t-self.seq_len:t]\n",
        "\n",
        "        # Stack per ottenere canali: (Seq, 3, 5, 9)\n",
        "        # Nota: PyTorch vuole i canali prima delle dimensioni spaziali\n",
        "        era5_seq = np.stack([u10_seq, v10_seq, msl_seq], axis=1)\n",
        "\n",
        "        return {\n",
        "            'eph': torch.FloatTensor(eph_seq),\n",
        "            'era5': torch.FloatTensor(era5_seq), # Shape: (48, 3, 5, 9)\n",
        "            'coords': torch.FloatTensor([self.lat[n], self.lon[n]]),\n",
        "            'target': torch.FloatTensor([self.wl[t, n]])\n",
        "        }\n",
        "train_end = 8760 * 9   # 9 anni di dati orari per il training\n",
        "val_start = train_end  # Il resto Ã¨ per la validazione\n",
        "# Rigenerazione Dataloader\n",
        "print(\"Creazione Dataset PRO (48h)...\")\n",
        "train_dataset = MemoryEfficientDataset(\n",
        "    eph_train[:train_end], train_u10_norm[:train_end], train_v10_norm[:train_end], train_msl_norm[:train_end],\n",
        "    wl_train[:train_end], lat_norm, lon_norm, seq_len=SEQ_LEN, nodes_per_timestep=NODES_PER_T\n",
        ")\n",
        "val_dataset = MemoryEfficientDataset(\n",
        "    eph_train[val_start:], train_u10_norm[val_start:], train_v10_norm[val_start:], train_msl_norm[val_start:],\n",
        "    wl_train[val_start:], lat_norm, lon_norm, seq_len=SEQ_LEN, nodes_per_timestep=NODES_PER_T * 2 # PiÃ¹ nodi in validazione\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "print(f\"Dataset pronto: {len(train_dataset)} training samples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLm7RZ3qLs_Q"
      },
      "source": [
        "## 5. Modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn0jIialLudU",
        "outputId": "4336c245-db96-47d8-914e-cc4eaf54fc9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello CNN-LSTM inizializzato.\n"
          ]
        }
      ],
      "source": [
        "class CNN_LSTM_Predictor(nn.Module):\n",
        "    def __init__(self, seq_len=48, hidden_size=128, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Encoder Spaziale (CNN per le mappe meteo)\n",
        "        # Input: (Batch*Seq, 3, 5, 9)\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1), # Esce (16, 5, 9)\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=(3,3), padding=0), # Esce (32, 3, 7)\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 3 * 7, 64), # Proietta a 64 features\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # 2. Encoder Effemeridi\n",
        "        self.eph_encoder = nn.Sequential(\n",
        "            nn.Linear(6, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # 3. LSTM Temporale\n",
        "        # Input size: 64 (da CNN) + 32 (da Eph) = 96\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=96,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # 4. Encoder Coordinate\n",
        "        self.coord_encoder = nn.Sequential(\n",
        "            nn.Linear(2, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # 5. Predittore Finale\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(hidden_size + 32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, eph_seq, era5_seq, coords):\n",
        "        # era5_seq shape: (Batch, Seq, 3, 5, 9)\n",
        "        B, T, C, H, W = era5_seq.shape\n",
        "\n",
        "        # Fondere Batch e Time per passare nella CNN\n",
        "        # (B*T, 3, 5, 9)\n",
        "        cnn_in = era5_seq.view(B * T, C, H, W)\n",
        "        cnn_out = self.cnn(cnn_in) # (B*T, 64)\n",
        "\n",
        "        # Ripristinare la dimensione temporale\n",
        "        # (B, T, 64)\n",
        "        cnn_features = cnn_out.view(B, T, -1)\n",
        "\n",
        "        # Processare effemeridi\n",
        "        eph_features = self.eph_encoder(eph_seq) # (B, T, 32)\n",
        "\n",
        "        # Concatenare feature CNN e Effemeridi\n",
        "        lstm_input = torch.cat([cnn_features, eph_features], dim=-1) # (B, T, 96)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(lstm_input)\n",
        "        last_hidden = lstm_out[:, -1, :] # Prendiamo l'ultimo stato (B, Hidden)\n",
        "\n",
        "        # Aggiungere coordinate spaziali del nodo\n",
        "        coord_feat = self.coord_encoder(coords) # (B, 32)\n",
        "\n",
        "        # Combinare tutto\n",
        "        combined = torch.cat([last_hidden, coord_feat], dim=-1)\n",
        "\n",
        "        return self.predictor(combined).squeeze(-1)\n",
        "\n",
        "# Reinizializza il modello\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CNN_LSTM_Predictor(seq_len=SEQ_LEN).to(device)\n",
        "print(\"Modello CNN-LSTM inizializzato.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XFOkI6RLv6h"
      },
      "source": [
        "## 6. Training con Gradient Accumulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9MNKhPQLxML",
        "outputId": "e214f66c-ac67-4c3b-ed02-74abc0455f3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configurazione:\n",
            "  Epoche: 50\n",
            "  LR: 0.001\n",
            "  Gradient accumulation: 2\n",
            "  Batch effettivo: 512\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "LEARNING_RATE = 0.001\n",
        "PATIENCE = 7\n",
        "ACCUM_STEPS = 2  # Accumula gradienti per simulare batch piÃ¹ grandi\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',\n",
        "                                                  factor=0.5, patience=3,\n",
        "                                                  min_lr=1e-6)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "print(f\"Configurazione:\")\n",
        "print(f\"  Epoche: {EPOCHS}\")\n",
        "print(f\"  LR: {LEARNING_RATE}\")\n",
        "print(f\"  Gradient accumulation: {ACCUM_STEPS}\")\n",
        "print(f\"  Batch effettivo: {BATCH_SIZE * ACCUM_STEPS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Lw60bmFLybV",
        "outputId": "ed7a6428-9ba7-45d3-dbe4-89e804d58097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Funzioni training pronte!\n"
          ]
        }
      ],
      "source": [
        "def train_epoch(model, loader, optimizer, criterion, device, accum_steps):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, batch in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
        "        eph = batch['eph'].to(device)\n",
        "        era5 = batch['era5'].to(device)\n",
        "        coords = batch['coords'].to(device)\n",
        "        targets = batch['target'].squeeze().to(device)\n",
        "\n",
        "        preds = model(eph, era5, coords)\n",
        "        loss = criterion(preds, targets)\n",
        "        loss = loss / accum_steps  # Scala loss\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % accum_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * accum_steps\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Validating\", leave=False):\n",
        "            eph = batch['eph'].to(device)\n",
        "            era5 = batch['era5'].to(device)\n",
        "            coords = batch['coords'].to(device)\n",
        "            targets = batch['target'].squeeze().to(device)\n",
        "\n",
        "            preds = model(eph, era5, coords)\n",
        "            loss = criterion(preds, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    preds_arr = np.concatenate(all_preds)\n",
        "    targets_arr = np.concatenate(all_targets)\n",
        "    rmse = np.sqrt(np.mean((preds_arr - targets_arr)**2))\n",
        "\n",
        "    return total_loss / len(loader), rmse\n",
        "\n",
        "print(\"âœ“ Funzioni training pronte!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6czMkCrLzrL"
      },
      "source": [
        "## 7. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fXfDCq7Oe--Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# --- STEP 1: Definizione cartella checkpoint ---\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/ProgettoDeepLearning/Modello2/checkpoints/'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# --- FUNZIONI DI SUPPORTO ---\n",
        "def save_checkpoint(epoch, model, optimizer, scheduler, train_losses, val_losses, val_rmses, best_val_rmse, checkpoint_dir):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'val_rmses': val_rmses,\n",
        "        'best_val_rmse': best_val_rmse\n",
        "    }\n",
        "    torch.save(checkpoint, os.path.join(checkpoint_dir, 'last_checkpoint.pth'))\n",
        "\n",
        "    # Se l'ultimo RMSE Ã¨ il migliore, salva anche come best_model\n",
        "    if val_rmses[-1] <= best_val_rmse:\n",
        "        torch.save(checkpoint, os.path.join(checkpoint_dir, 'best_model.pth'))\n",
        "        print(f\"  âœ“ Modello salvato come MIGLIORE in: {checkpoint_dir}\")\n",
        "\n",
        "def load_checkpoint(model, optimizer, scheduler, checkpoint_dir):\n",
        "    path = os.path.join(checkpoint_dir, 'last_checkpoint.pth')\n",
        "    if not os.path.exists(path):\n",
        "        print(\"âš ï¸ Nessun checkpoint trovato. Inizio da zero.\")\n",
        "        return 0, [], [], [], float('inf')\n",
        "\n",
        "    print(f\"âœ… Caricamento checkpoint da {path}...\")\n",
        "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(ckpt['model_state_dict'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
        "    return ckpt['epoch'] + 1, ckpt['train_losses'], ckpt['val_losses'], ckpt['val_rmses'], ckpt['best_val_rmse']\n",
        "\n",
        "def load_best_model(model, checkpoint_dir):\n",
        "    path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
        "    if os.path.exists(path):\n",
        "        ckpt = torch.load(path)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        print(\"âœ… Caricato il miglior modello per il test.\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Nessun 'best_model' trovato, uso i pesi attuali.\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwzZ5l6LL1Hb",
        "outputId": "e14b3403-2246-43f5-bd6b-838e23b6df6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Caricamento checkpoint da /content/drive/MyDrive/ProgettoDeepLearning/Modello2/checkpoints/last_checkpoint.pth...\n",
            "\n",
            "Inizio training dall'epoca 11...\n",
            "\n",
            "Epoch 11/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train: 0.002492\n",
            "  Val: 0.007890, RMSE: 0.0888\n",
            "  LR: 0.000500\n",
            "  No improvement (1/7)\n",
            "Epoch 12/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train: 0.002471\n",
            "  Val: 0.007831, RMSE: 0.0885\n",
            "  LR: 0.000500\n",
            "  No improvement (2/7)\n",
            "Epoch 13/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train: 0.002458\n",
            "  Val: 0.007888, RMSE: 0.0888\n",
            "  LR: 0.000500\n",
            "  No improvement (3/7)\n",
            "Epoch 14/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train: 0.002450\n",
            "  Val: 0.007657, RMSE: 0.0875\n",
            "  LR: 0.000500\n",
            "  âœ“ Modello salvato come MIGLIORE in: /content/drive/MyDrive/ProgettoDeepLearning/Modello2/checkpoints/\n",
            "  âœ“ NUOVO RECORD! 0.0875\n",
            "Epoch 15/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train: 0.002438\n",
            "  Val: 0.008089, RMSE: 0.0899\n",
            "  LR: 0.000500\n",
            "  No improvement (1/7)\n",
            "Epoch 16/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train: 0.002435\n",
            "  Val: 0.007749, RMSE: 0.0880\n",
            "  LR: 0.000500\n",
            "  No improvement (2/7)\n",
            "Epoch 17/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train: 0.002424\n",
            "  Val: 0.008266, RMSE: 0.0909\n",
            "  LR: 0.000500\n",
            "  No improvement (3/7)\n",
            "Epoch 18/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train: 0.002420\n",
            "  Val: 0.007761, RMSE: 0.0881\n",
            "  LR: 0.000250\n",
            "  No improvement (4/7)\n",
            "Epoch 19/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train: 0.002341\n",
            "  Val: 0.007699, RMSE: 0.0877\n",
            "  LR: 0.000250\n",
            "  No improvement (5/7)\n",
            "Epoch 20/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train: 0.002332\n",
            "  Val: 0.007879, RMSE: 0.0888\n",
            "  LR: 0.000250\n",
            "  No improvement (6/7)\n",
            "Epoch 21/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train: 0.002325\n",
            "  Val: 0.007658, RMSE: 0.0875\n",
            "  LR: 0.000250\n",
            "  No improvement (7/7)\n",
            "\\nEarly stopping @ 21\n",
            "\n",
            "============================================================\n",
            "Best val RMSE: 0.0875\n",
            "Baseline: 0.0859\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# --- STEP 2: Prima del training loop ---\n",
        "# Carica stato precedente se esiste\n",
        "start_epoch, train_losses, val_losses, val_rmses, best_val_rmse = load_checkpoint(\n",
        "    model, optimizer, scheduler, CHECKPOINT_DIR\n",
        ")\n",
        "patience_counter = 0\n",
        "\n",
        "print(f\"\\nInizio training dall'epoca {start_epoch + 1}...\\n\")\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS): # Usa start_epoch per riprendere correttamente\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion,\n",
        "                            device, ACCUM_STEPS)\n",
        "    val_loss, val_rmse = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_rmses.append(val_rmse)\n",
        "\n",
        "    scheduler.step(val_rmse)\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    print(f\"  Train: {train_loss:.6f}\")\n",
        "    print(f\"  Val: {val_loss:.6f}, RMSE: {val_rmse:.4f}\")\n",
        "    print(f\"  LR: {lr:.6f}\")\n",
        "\n",
        "    # --- STEP 3: Nel training loop (dopo ogni epoca) ---\n",
        "    save_checkpoint(epoch, model, optimizer, scheduler,\n",
        "                    train_losses, val_losses, val_rmses,\n",
        "                    best_val_rmse, CHECKPOINT_DIR)\n",
        "\n",
        "    if val_rmse < best_val_rmse:\n",
        "        best_val_rmse = val_rmse\n",
        "        print(f\"  âœ“ NUOVO RECORD! {val_rmse:.4f}\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"  No improvement ({patience_counter}/{PATIENCE})\")\n",
        "\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(f\"\\\\nEarly stopping @ {epoch+1}\")\n",
        "        break\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Best val RMSE: {best_val_rmse:.4f}\")\n",
        "print(f\"Baseline: {baseline_rmse:.4f}\")\n",
        "if best_val_rmse < baseline_rmse:\n",
        "    print(f\"Miglioramento: +{(baseline_rmse-best_val_rmse)/baseline_rmse*100:.2f}%\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FINE TUNING CHIRURGICO (FIXED) ---\n",
        "import os\n",
        "\n",
        "print(\"RECUPERO TRAINING: Caricamento Best Model (0.0875)...\")\n",
        "\n",
        "# --- FIX PER L'ERRORE PICKLE ---\n",
        "# Carichiamo manualmente con weights_only=False perchÃ© ci fidiamo del file\n",
        "path_best = os.path.join(CHECKPOINT_DIR, 'last_checkpoint.pth') # Usiamo last_checkpoint che ha lo stato piÃ¹ recente\n",
        "\n",
        "if os.path.exists(path_best):\n",
        "    # QUI c'era l'errore: aggiungiamo weights_only=False\n",
        "    ckpt = torch.load(path_best, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(ckpt['model_state_dict'])\n",
        "    print(f\"âœ… Pesi caricati correttamente! Ripartiamo dall'RMSE: {ckpt.get('best_val_rmse', 0.0875):.4f}\")\n",
        "else:\n",
        "    print(\"âš ï¸ Checkpoint non trovato, uso i pesi attuali in memoria.\")\n",
        "\n",
        "# Impostiamo un LR molto basso per la \"limatura\" finale\n",
        "FINE_TUNE_LR = 0.0001\n",
        "optimizer = optim.AdamW(model.parameters(), lr=FINE_TUNE_LR, weight_decay=1e-5)\n",
        "\n",
        "# 20 epoche extra\n",
        "EXTRA_EPOCHS = 20\n",
        "best_rmse_finetune = 0.0875  # Il tuo record attuale\n",
        "\n",
        "print(f\"\\nInizio Fine-Tuning per battere 0.0859...\")\n",
        "print(f\"Target da battere: {baseline_rmse:.4f}\\n\")\n",
        "\n",
        "for epoch in range(EXTRA_EPOCHS):\n",
        "    print(f\"Fine-Tune Epoch {epoch+1}/{EXTRA_EPOCHS}\")\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for i, batch in enumerate(tqdm(train_loader, desc=\"Fine-tuning\", leave=False)):\n",
        "        eph = batch['eph'].to(device)\n",
        "        era5 = batch['era5'].to(device)\n",
        "        coords = batch['coords'].to(device)\n",
        "        targets = batch['target'].squeeze().to(device)\n",
        "\n",
        "        preds = model(eph, era5, coords)\n",
        "        loss = criterion(preds, targets)\n",
        "        loss = loss / ACCUM_STEPS\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % ACCUM_STEPS == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item() * ACCUM_STEPS\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_rmse = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"  Train: {avg_train_loss:.6f}\")\n",
        "    print(f\"  Val RMSE: {val_rmse:.4f} (Best: {best_rmse_finetune:.4f})\")\n",
        "\n",
        "    # Salvataggio manuale se miglioriamo\n",
        "    if val_rmse < best_rmse_finetune:\n",
        "        best_rmse_finetune = val_rmse\n",
        "        # Salviamo anche qui con il nome speciale per non sovrascrivere l'originale se non vogliamo\n",
        "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_model_finetuned.pth'))\n",
        "        print(f\"  ðŸš€ NUOVO RECORD ASSOLUTO! {val_rmse:.4f}\")\n",
        "\n",
        "        if val_rmse < baseline_rmse:\n",
        "            print(f\"  ðŸŽ‰ BASELINE BATTUTA! ({val_rmse:.4f} < {baseline_rmse:.4f})\")\n",
        "\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KdEqqb9uOaE",
        "outputId": "7f97dc2d-7a1d-4cf5-80b9-e274e19d30f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RECUPERO TRAINING: Caricamento Best Model (0.0875)...\n",
            "âœ… Pesi caricati correttamente! Ripartiamo dall'RMSE: 0.0875\n",
            "\n",
            "Inizio Fine-Tuning per battere 0.0859...\n",
            "Target da battere: 0.0859\n",
            "\n",
            "Fine-Tune Epoch 1/20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train: 0.002279\n",
            "  Val RMSE: 0.0872 (Best: 0.0875)\n",
            "  ðŸš€ NUOVO RECORD ASSOLUTO! 0.0872\n",
            "----------------------------------------\n",
            "Fine-Tune Epoch 2/20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train: 0.002272\n",
            "  Val RMSE: 0.0880 (Best: 0.0872)\n",
            "----------------------------------------\n",
            "Fine-Tune Epoch 3/20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train: 0.002269\n",
            "  Val RMSE: 0.0881 (Best: 0.0872)\n",
            "----------------------------------------\n",
            "Fine-Tune Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train: 0.002269\n",
            "  Val RMSE: 0.0885 (Best: 0.0872)\n",
            "----------------------------------------\n",
            "Fine-Tune Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tuning:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 11067/15390 [13:00<05:30, 13.09it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMSweFW6L2OR"
      },
      "source": [
        "### Grafici"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuqId24TL3fT"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(train_losses, 'b-', linewidth=2, label='Train')\n",
        "axes[0].plot(val_losses, 'orange', linewidth=2, label='Val')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].plot(val_rmses, 'g-', linewidth=2, label='Val RMSE')\n",
        "axes[1].axhline(baseline_rmse, color='r', linestyle='--', linewidth=2, label='Baseline')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('RMSE')\n",
        "axes[1].set_title('Validation RMSE')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best: {min(val_rmses):.4f} @ epoch {val_rmses.index(min(val_rmses))+1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pWOHPxcL4po"
      },
      "source": [
        "## 8. Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZU2ZtGQL56m"
      },
      "outputs": [],
      "source": [
        "# 1. Carica il MIGLIOR modello salvato durante il training\n",
        "# Usa la funzione helper che gestisce percorso e dizionario correttamente\n",
        "model = load_best_model(model, CHECKPOINT_DIR)\n",
        "model.eval()\n",
        "\n",
        "print(\"âœ“ Best model caricato! Procedo col test...\\n\")\n",
        "\n",
        "# Dataset test (campiona meno nodi per memoria)\n",
        "test_dataset = MemoryEfficientDataset(\n",
        "    eph_test,\n",
        "    test_u10_norm,\n",
        "    test_v10_norm,\n",
        "    test_msl_norm,\n",
        "    wl_test,\n",
        "    lat_norm,\n",
        "    lon_norm,\n",
        "    seq_len=SEQ_LEN,\n",
        "    nodes_per_timestep=1000  # PiÃ¹ nodi per test accurato\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                        shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Test samples: {len(test_dataset):,}\\n\")\n",
        "\n",
        "# Predizione\n",
        "test_preds = []\n",
        "test_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        eph = batch['eph'].to(device)\n",
        "        era5 = batch['era5'].to(device)\n",
        "        coords = batch['coords'].to(device)\n",
        "        targets = batch['target'].squeeze().to(device)\n",
        "\n",
        "        preds = model(eph, era5, coords)\n",
        "        test_preds.append(preds.cpu().numpy())\n",
        "        test_true.append(targets.cpu().numpy())\n",
        "\n",
        "test_preds = np.concatenate(test_preds)\n",
        "test_true = np.concatenate(test_true)\n",
        "\n",
        "final_rmse = np.sqrt(np.mean((test_preds - test_true)**2))\n",
        "final_mae = np.mean(np.abs(test_preds - test_true))\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"TEST 2020 - RISULTATI FINALI\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"RMSE: {final_rmse:.4f}\")\n",
        "print(f\"MAE:  {final_mae:.4f}\")\n",
        "print(f\"Baseline: {baseline_rmse:.4f}\")\n",
        "if final_rmse < baseline_rmse:\n",
        "    print(f\"Miglioramento: +{(baseline_rmse-final_rmse)/baseline_rmse*100:.2f}% âœ“\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKRuTWVwL6fE"
      },
      "source": [
        "## 9. Visualizzazioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H273Fx9ZL9Uu"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Scatter\n",
        "idx = np.random.choice(len(test_preds), min(2000, len(test_preds)), False)\n",
        "axes[0,0].scatter(test_true[idx], test_preds[idx], alpha=0.3, s=2, c='blue')\n",
        "axes[0,0].plot([test_true.min(), test_true.max()],\n",
        "              [test_true.min(), test_true.max()], 'r--', lw=2)\n",
        "axes[0,0].set_xlabel('True')\n",
        "axes[0,0].set_ylabel('Predicted')\n",
        "axes[0,0].set_title('Predictions vs Reality')\n",
        "axes[0,0].grid(alpha=0.3)\n",
        "\n",
        "# Errori\n",
        "errors = test_preds - test_true\n",
        "axes[0,1].hist(errors, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
        "axes[0,1].axvline(0, color='r', linestyle='--', lw=2)\n",
        "axes[0,1].set_xlabel('Error')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "axes[0,1].set_title(f'Error Distribution (std={errors.std():.4f})')\n",
        "axes[0,1].grid(alpha=0.3)\n",
        "\n",
        "# Time series esempio\n",
        "axes[1,0].plot(test_true[:200], 'b-', lw=1.5, alpha=0.8, label='True')\n",
        "axes[1,0].plot(test_preds[:200], 'orange', lw=1.5, alpha=0.8, label='Pred')\n",
        "axes[1,0].set_xlabel('Sample')\n",
        "axes[1,0].set_ylabel('Water Level')\n",
        "axes[1,0].set_title('Time Series Example')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(alpha=0.3)\n",
        "\n",
        "# Errore nel tempo\n",
        "abs_err = np.abs(errors)\n",
        "axes[1,1].plot(abs_err[:500], 'g-', lw=1)\n",
        "axes[1,1].axhline(final_mae, color='r', linestyle='--', lw=2, label=f'MAE={final_mae:.4f}')\n",
        "axes[1,1].set_xlabel('Sample')\n",
        "axes[1,1].set_ylabel('Absolute Error')\n",
        "axes[1,1].set_title('Error Over Time')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_results.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ“ Fatto!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}